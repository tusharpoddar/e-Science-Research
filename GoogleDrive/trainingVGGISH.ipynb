{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trainingVGGISH.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1eCrUIvtmlQMUfysu9ckyW9_Tf4sCQCty",
      "authorship_tag": "ABX9TyNlir3zGT/l03I9ufHw0rES",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tusharpoddar/e-Science-Research/blob/master/GoogleDrive/trainingVGGISH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd08zRPoh7Co",
        "colab_type": "text"
      },
      "source": [
        "# The following notebook is used to train the VGG-model with the netcdf files that have been generated.\n",
        "\n",
        "The notebook does the following things -\n",
        "1. Used the data in the numpy format(creating it once again during training takes a lot of time).\n",
        "2. Training the whole data of 1700 data points with single batch training for 10 times.\n",
        "3. The notebook then plots the loss(cross entropy) after each epoch. \n",
        "\n",
        "## Data that is being used in the notebook -\n",
        "The notebook loads two numpy files that contain the spectrogram data and the labels. This data has 1700 data points in total. \n",
        "1. Training Data - this set contins 1600 data points(randomly chosen). These are used for the training.\n",
        "\n",
        "## Next step - \n",
        "The notebook does not test the performace of the model. So this can be seen as a dummy example of how to run the training of the model after passing. it some data. Proper validation and evaluation of the performance of the model is done in the next notebook called Single Prediction training. \n",
        "\n",
        "Note the label [1, 0] represents mice noise and [0, 1] represents random noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGcoksb-nxMG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0e0b053d-64ef-47f1-bc86-2e1c22328677"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive' , force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFaO2_rAfxvI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "850312ab-10a0-4019-e276-8e0d37b2a6e1"
      },
      "source": [
        "!pip install numpy resampy tensorflow==1.15 tf_slim six soundfile"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: resampy in /usr/local/lib/python3.6/dist-packages (0.2.2)\n",
            "Collecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 36kB/s \n",
            "\u001b[?25hCollecting tf_slim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 37.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (1.12.0)\n",
            "Collecting soundfile\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numba>=0.32 in /usr/local/lib/python3.6/dist-packages (from resampy) (0.48.0)\n",
            "Requirement already satisfied: scipy>=0.13 in /usr/local/lib/python3.6/dist-packages (from resampy) (1.4.1)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 48.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.12.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.30.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.9.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.2.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.34.2)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 47.3MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.14.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.32->resampy) (49.1.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.32->resampy) (0.31.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.2.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=707288f3189d53841c74d670431e62ed2b3d5819311e763fd5d0a5642fb6cedd\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, gast, tensorflow, tf-slim, soundfile\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "Successfully installed gast-0.2.2 soundfile-0.10.3.post1 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1 tf-slim-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSCGVfAejrVX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "9fb3c115-75a5-49c7-c3af-0c2ee48d1f49"
      },
      "source": [
        "pip install netcdf4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting netcdf4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/4f/d49fe0c65dea4d2ebfdc602d3e3d2a45a172255c151f4497c43f6d94a5f6/netCDF4-1.5.3-cp36-cp36m-manylinux1_x86_64.whl (4.1MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1MB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from netcdf4) (1.18.5)\n",
            "Collecting cftime\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/0f/846488085d0f5517d79dfd7a12cd231ff87b94265a5bbfef62da56a6b029/cftime-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (282kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 40.5MB/s \n",
            "\u001b[?25hInstalling collected packages: cftime, netcdf4\n",
            "Successfully installed cftime-1.2.0 netcdf4-1.5.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqdrqI6_jsga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to open the selection tables\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URc_IaBQj0hb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/gdrive/Shared drives/USV_eScience_Incubator/Data/feature_data_frames/Old/annot_8features_100noise_fear.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxkXDBQzj3ON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "annot = pd.read_csv(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyHQkLUaj38p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "b738c6c3-1174-467a-f46a-22877bbebcdd"
      },
      "source": [
        "annot = pd.DataFrame(data = annot)\n",
        "print(annot)\n",
        "print(annot.Annotation.value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Unnamed: 0  animal_number   session  ...  spec_kurt spec_slope     spec_roll\n",
            "0              0            533   CPApair  ...  26.730238   0.000149  38781.012731\n",
            "1              1            533   CPApair  ...  50.458618  -0.000471  37262.344520\n",
            "2              2            533   CPApair  ...  35.797254  -0.001451  14898.363521\n",
            "3              3            533   CPApair  ...  25.973715  -0.000857  24369.181905\n",
            "4              4            533   CPApair  ...   4.124133  -0.001831  19606.292614\n",
            "...          ...            ...       ...  ...        ...        ...           ...\n",
            "1754        1595            557  cagepair  ...   0.945694  -0.000005  26382.647835\n",
            "1755        1596            557  cagepair  ...   1.348464  -0.000287  24299.949586\n",
            "1756        1597            557  cagepair  ...   5.296944  -0.001228  19409.636199\n",
            "1757        1598            557  cagepair  ...   0.873569   0.000367  29020.685860\n",
            "1758        1599            557  cagepair  ...   1.744215  -0.000575  26036.829758\n",
            "\n",
            "[1759 rows x 13 columns]\n",
            "rand_noise    1600\n",
            "low slug        60\n",
            "bbc             45\n",
            "high slug       27\n",
            "low multi       23\n",
            "high multi       4\n",
            "Name: Annotation, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5mvJDlNj73M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "annot_np = np.array(annot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awo4T-nbqACE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# opeing a data file to check if the time stamps exists or not \n",
        "path = '/content/gdrive/Shared drives/USV_eScience_Incubator/Data/netcdf_files/Old/Fear/533_CPApair_xr_Dataset.nc'\n",
        "data = xr.open_dataset(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGCcO5Afkau0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to produce the path of the netcdf file that is specified in the selection table\n",
        "def pathGen(animalNumber, experimentName):\n",
        "  return '/content/gdrive/Shared drives/USV_eScience_Incubator/Data/netcdf_files/Old/Fear/' + str(animalNumber)+'_' +str(experimentName) + '_xr_Dataset.nc'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RDxW5ErkSnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the following function uses the selection table to generate 2 arrays - \n",
        "# the numpy array that represents the spectrogram information and\n",
        "# the other that helps us to differentiate between the mice noise and random noise\n",
        "def prepare_data(annot_np):\n",
        "  dataArray = []\n",
        "  labelArray = []\n",
        "  for record in annot_np:\n",
        "    animalNumber = record[1]\n",
        "    experimentName = record[2]\n",
        "    time = record[3]\n",
        "    path = pathGen(animalNumber, experimentName)\n",
        "    print(path)\n",
        "    data = xr.open_dataset(path)\n",
        "  \n",
        "    print(time)\n",
        "    xr_slice = data['__xarray_dataarray_variable__'].sel(slices=time)\n",
        "    temp = np.array(xr_slice)\n",
        "    # in order to change the shape of the spectrogram so that it matches the format meeded for by the vggish model\n",
        "    changed = cv2.resize(temp, (64, 96))\n",
        "    dataArray.append(changed)\n",
        "    if record[4] == 'rand_noise':\n",
        "      labelArray.append([0, 1])\n",
        "    else:\n",
        "      labelArray.append([1, 0])\n",
        "\n",
        "  # once both the arrays are formed i need to convert them to numpy array and then return them\n",
        "  numpyData = np.array(dataArray)\n",
        "  numpyLabel = np.array(labelArray)\n",
        "  return numpyData, numpyLabel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIoeDpu_19zP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c0273d87-ec5a-40f5-9458-26bc4d1d955d"
      },
      "source": [
        "cd /content/gdrive/Shared drives/Research"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/Shared drives/Research\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkq_xRhAfnSz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "5b5b7a66-eb5b-4a8e-de43-b34cf8edcf99"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from random import shuffle\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import tf_slim as slim\n",
        "\n",
        "import vggish_input\n",
        "import vggish_params\n",
        "import vggish_slim\n",
        "\n",
        "flags = tf.app.flags"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbqcDFCHg4mt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flags = tf.app.flags\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    'num_batches', 10,\n",
        "    'Number of batches of examples to feed into the model. Each batch is of '\n",
        "    'variable size and contains shuffled examples of each class of audio.')\n",
        "\n",
        "flags.DEFINE_boolean(\n",
        "    'train_vggish', True,\n",
        "    'If True, allow VGGish parameters to change during training, thus '\n",
        "    'fine-tuning VGGish. If False, VGGish parameters are fixed, thus using '\n",
        "    'VGGish as a fixed feature extractor.')\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    'checkpoint', 'vggish_model.ckpt',\n",
        "    'Path to the VGGish checkpoint file.')\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "_NUM_CLASSES = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcslGkChh0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.app.flags.DEFINE_string('f', '', 'kernel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6468mfNfrVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _get_examples_batch(numpyData, numpyLabel):\n",
        "  \"\"\"Returns a shuffled batch of examples of all audio classes.\n",
        "\n",
        "  Note that this is just a toy function because this is a simple demo intended\n",
        "  to illustrate how the training code might work.\n",
        "\n",
        "  Returns:\n",
        "    a tuple (features, labels) where features is a NumPy array of shape\n",
        "    [batch_size, num_frames, num_bands] where the batch_size is variable and\n",
        "    each row is a log mel spectrogram patch of shape [num_frames, num_bands]\n",
        "    suitable for feeding VGGish, while labels is a NumPy array of shape\n",
        "    [batch_size, num_classes] where each row is a multi-hot label vector that\n",
        "    provides the labels for corresponding rows in features.\n",
        "  \"\"\"\n",
        "  # Shuffle (example, label) pairs across all classes.\n",
        "  labeled_examples = list(zip(numpyData, numpyLabel))\n",
        "  shuffle(labeled_examples)\n",
        "\n",
        "  # Separate and return the features and labels.\n",
        "  features = [example for (example, _) in labeled_examples]\n",
        "  labels = [label for (_, label) in labeled_examples]\n",
        "  return (features, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOK0fvsW1JLk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "e9f4af12-99ab-48e1-f872-842dbfd27aa2"
      },
      "source": [
        "with tf.Graph().as_default(), tf.Session() as sess:\n",
        "    # Define VGGish.\n",
        "    embeddings = vggish_slim.define_vggish_slim(FLAGS.train_vggish)\n",
        "\n",
        "    # Define a shallow classification model and associated training ops on top\n",
        "    # of VGGish.\n",
        "    with tf.variable_scope('mymodel'):\n",
        "      # Add a fully connected layer with 100 units.\n",
        "      num_units = 100\n",
        "      fc = slim.fully_connected(embeddings, num_units)\n",
        "\n",
        "      # Add a classifier layer at the end, consisting of parallel logistic\n",
        "      # classifiers, one per class. This allows for multi-class tasks.\n",
        "      logits = slim.fully_connected(\n",
        "          fc, _NUM_CLASSES, activation_fn=None, scope='logits')\n",
        "      tf.sigmoid(logits, name='prediction')\n",
        "\n",
        "      # Add training ops.\n",
        "      with tf.variable_scope('train'):\n",
        "        global_step = tf.Variable(\n",
        "            0, name='global_step', trainable=False,\n",
        "            collections=[tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                         tf.GraphKeys.GLOBAL_STEP])\n",
        "\n",
        "        # Labels are assumed to be fed as a batch multi-hot vectors, with\n",
        "        # a 1 in the position of each positive class label, and 0 elsewhere.\n",
        "        labels = tf.placeholder(\n",
        "            tf.float32, shape=(None, _NUM_CLASSES), name='labels')\n",
        "\n",
        "        # Cross-entropy label loss.\n",
        "        xent = tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits=logits, labels=labels, name='xent')\n",
        "        loss = tf.reduce_mean(xent, name='loss_op')\n",
        "        tf.summary.scalar('loss', loss)\n",
        "\n",
        "        # We use the same optimizer and hyperparameters as used to train VGGish.\n",
        "        optimizer = tf.train.AdamOptimizer(\n",
        "            learning_rate=vggish_params.LEARNING_RATE,\n",
        "            epsilon=vggish_params.ADAM_EPSILON)\n",
        "        optimizer.minimize(loss, global_step=global_step, name='train_op')\n",
        "\n",
        "    # Initialize all variables in the model, and then load the pre-trained\n",
        "    # VGGish checkpoint.\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    vggish_slim.load_vggish_slim_checkpoint(sess, FLAGS.checkpoint)\n",
        "\n",
        "    # Locate all the tensors and ops we need for the training loop.\n",
        "    features_tensor = sess.graph.get_tensor_by_name(\n",
        "        vggish_params.INPUT_TENSOR_NAME)\n",
        "    labels_tensor = sess.graph.get_tensor_by_name('mymodel/train/labels:0')\n",
        "    global_step_tensor = sess.graph.get_tensor_by_name(\n",
        "        'mymodel/train/global_step:0')\n",
        "    loss_tensor = sess.graph.get_tensor_by_name('mymodel/train/loss_op:0')\n",
        "    train_op = sess.graph.get_operation_by_name('mymodel/train/train_op')\n",
        "    # calculating the numpy array only once\n",
        "    \n",
        "    #numpyData, numpyLabel = prepare_data(annot_np)\n",
        "    \n",
        "    numpyData = np.load('/content/gdrive/Shared drives/Research/Data/Numpy files/annot_df_CPApost_round2/fearNoiseSpectrogram.npy')\n",
        "    numpyLabel = np.load('/content/gdrive/Shared drives/Research/Data/Numpy files/annot_df_CPApost_round2/fearNoiseLabel.npy')\n",
        "    # The training loop.\n",
        "    for _ in range(FLAGS.num_batches):\n",
        "      (features, labels) = _get_examples_batch(numpyData, numpyLabel)\n",
        "      [num_steps, loss, _] = sess.run(\n",
        "          [global_step_tensor, loss_tensor, train_op],\n",
        "          feed_dict={features_tensor: features, labels_tensor: labels})\n",
        "      print('Step %d: loss %g' % (num_steps, loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1666: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
            "Step 1: loss 2.91402\n",
            "Step 2: loss 2.3604\n",
            "Step 3: loss 1.91897\n",
            "Step 4: loss 1.57996\n",
            "Step 5: loss 1.32861\n",
            "Step 6: loss 1.15649\n",
            "Step 7: loss 1.04067\n",
            "Step 8: loss 0.949387\n",
            "Step 9: loss 0.871084\n",
            "Step 10: loss 0.807416\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}