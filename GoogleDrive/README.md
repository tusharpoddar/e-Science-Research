# New jupyter notebooks

Note: The following notebooks use specific data files in the USV Shared Drive

Detailed explanition of all the notebooks and the order in which they need to be seen is specified below.

Link to all the data files that are used in these notebooks -
https://drive.google.com/drive/folders/1_y0bo4ju76764ua5MwWs5E5toN5YbJiO?usp=sharing

In the following research we are using the VGGish model to analyse the sounds generated by mice and then classifying them into varied feeling(fear, pain, etc) that the mice posses at different times.

Jupyter notebooks -

1. Git-vs-VGG-spectrogram - The following jupyter notebooks compares the spectrograms that are made using VGGish model and the ones that are made using scipy spectrogram function(named it Git becuause one of our collaborators used it previosuly which is now available on GitHub). With this we came to a conclusion that the spectograms made by the scipy function gave better results.

2. Interpolated Spectrogram - The data in the NetCDF file is of the form (26666, 55, 257). Where 55 represents the time stamps and 257 represents the frequency stamps. In order for this data to be used by the VGGish model we need to convert it in the form of (?, 96, 64). Where 96 represent the time stamps and 64 represents the frequency stamps. This notebook explains how to convert the data from the netCDF format to the numpy format so that it can be used by the VGGish model.

3. Netcdf-to-numpy(no-rand) - The following file reads a specified csv file that has the information like - experiment, animan number, time stamps. The notebook then looks at that particular time stamp in the specified NetCDF file. It then changes its shape so that it can be used by the VGGish model and then stores it in a numpy file called - annot_df_CPApost_round2_data.npy

4. Netcdf-to-numpy - The following file does the same thing that the previous file does but with one extra feature. It now works with two numpy arrays. It stores the data of the mice sound in one and the random noise in the another. We later use there files to plot the cluster plot and the interactive plots.

5. Mice-Noise-Visualization - This file works with the numpy files for the mice sound and random noise that were generated in the above jupyter notebook. The notebook uses holoviews to allow the user to select the category they want to observe(mice sounds or random sound) and then scroll through multiple spectogrograms for each category to understand the difference between the different categories of sound.

6. tf-record-generator - In order to plot the cluster plot it is important to extract the tf-records(they provide us with the location of the spectrogram). This file shows how to generate the tf-records for a numpy file that has been generated before.

7. Cluster Plot - Most simple cluster plot that plots 6 sounds generated by mice and 6 random noise together to see if the matplotlib offsetbox is able to distinguish between them. The following code used the tf-records and the numpy array for the offsetbox plot.

8. Cluster-plot-netcdf-file - Since the previos plot plotted just 12 spectrograms in total we needed more data points to really appreciate the clustering. To take that into consideration this notebooks incorporated data from one selection table that has 1790 data points. It then plots the cluster plot. Since plotting all the spectograms does not help us in identifying the type of sound that it plots. So instead of showing the spectorograms, it shows small squared boxes where different colors represent different types of sound plotted(mice - BBC, low slug, etc and random noise).

9. trainingVGGISH - This notebook is used to train the numpy data that has been generated from the selection table in the notebook Netcdf-to-numpy. The notebook defines the tensorflow model that uses the cross entropy function for minimizing the loss and the Adam optimizer for the training. The notebook only calculated the loss for 10 batch trainings. The notebook, however, does not plot the confusion matrix and the ROC curve to let us evaluate the performance of the model that we made.

10. Single-prediction-training - The following notebook is used to train the model with the dataset that was being used in the previous notebook. But this time the notebook also calculates the accurancy of the model. It also plots the confusion matrix based on the testing done on a dataset of about 150 data points(10 percent).

11. two-dataset-prediction-training - Since the results in the previous training were not the best we though that maybe testing the model with more data will imporove the performance of the model. Now, this files takes into account 2 datasets. So the total number of data-points are about 3700. The notebook also plots the ROC curve for the trained model for both the prediction classes.
